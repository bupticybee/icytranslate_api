{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('align_and_translate_char_50000/dic.pkl','rb') as fhdl:\n",
    "    ( \n",
    "        ind2ch,\n",
    "        ch2ind,\n",
    "        ind2en,\n",
    "        en2ind,\n",
    "    ) = pickle.load(fhdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_vocab_size = 50003#len(ind2en) + 3\n",
    "target_vocat_size = 8826#len(ind2ch) + 3\n",
    "attention_hidden_size = 512\n",
    "attention_output_size = 512\n",
    "embedding_size = 512\n",
    "seq_max_len = 40\n",
    "num_units = 512\n",
    "batch_size = 4#64\n",
    "layer_number = 2\n",
    "max_grad = 1.0\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers import core as layers_core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(log_device_placement=True,allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "\n",
    "with tf.device('/gpu:1'):\n",
    "    initializer = tf.random_uniform_initializer(\n",
    "        -0.08, 0.08)\n",
    "    tf.get_variable_scope().set_initializer(initializer)\n",
    "    \n",
    "    x = tf.placeholder(\"int32\", [None, None])\n",
    "    y = tf.placeholder(\"int32\", [None, None])\n",
    "    y_in = tf.placeholder(\"int32\",[None,None])\n",
    "    x_len = tf.placeholder(\"int32\",[None])\n",
    "    y_len = tf.placeholder(\"int32\",[None])\n",
    "    x_real_len = tf.placeholder(\"int32\",[None])\n",
    "    y_real_len = tf.placeholder(\"int32\",[None])\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "    \n",
    "    # embedding\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", [src_vocab_size, embedding_size],dtype=tf.float32)\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", [target_vocat_size, embedding_size],dtype=tf.float32)\n",
    "    \n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder, x)\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder, y_in)\n",
    "    \n",
    "    # encoder\n",
    "    num_bi_layers = int(layer_number / 2)\n",
    "    cell_list = []\n",
    "    for i in range(num_bi_layers):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        encoder_cell = cell_list[0]\n",
    "    else:\n",
    "        encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "        \n",
    "    cell_list = []\n",
    "    \n",
    "    for i in range(num_bi_layers):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        encoder_backword_cell = cell_list[0]\n",
    "    else:\n",
    "        encoder_backword_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "    \n",
    "    bi_outputs, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        encoder_cell,encoder_backword_cell, encoder_emb_inp,\n",
    "        sequence_length=x_len,dtype=tf.float32)\n",
    "    encoder_outputs = tf.concat(bi_outputs, -1)\n",
    "    \n",
    "    if num_bi_layers == 1:\n",
    "        encoder_state = bi_encoder_state\n",
    "    else:\n",
    "        encoder_state = []\n",
    "        for layer_id in range(num_bi_layers):\n",
    "            encoder_state.append(bi_encoder_state[0][layer_id])  # forward\n",
    "            encoder_state.append(bi_encoder_state[1][layer_id])  # backward\n",
    "        encoder_state = tuple(encoder_state)\n",
    "    \n",
    "    # decoder \n",
    "    #decoder_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    cell_list = []\n",
    "    for i in range(layer_number):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        decoder_cell = cell_list[0]\n",
    "    else:\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "    \n",
    "    # Helper\n",
    "    \n",
    "    # attention\n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        attention_hidden_size, encoder_outputs,\n",
    "        memory_sequence_length=x_real_len,scale=True)\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        decoder_cell, attention_mechanism,\n",
    "        attention_layer_size=attention_output_size)\n",
    "    \n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        target_vocat_size, use_bias=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Dynamic decoding\n",
    "    with tf.variable_scope(\"decode_layer\"):\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            decoder_emb_inp,sequence_length= y_len)\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper, initial_state = decoder_cell.zero_state(dtype=tf.float32,batch_size=batch_size),\n",
    "            output_layer=projection_layer)\n",
    "       \n",
    "        outputs, _,___  = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "        logits = outputs.rnn_output\n",
    "\n",
    "        target_weights = tf.sequence_mask(\n",
    "            y_real_len, seq_max_len, dtype=logits.dtype)\n",
    "    \n",
    "    # predicting\n",
    "    # Helper\n",
    "    with tf.variable_scope(\"decode_layer\", reuse=True):\n",
    "        helper_predict = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding_decoder,\n",
    "            tf.fill([batch_size], ch2ind['<go>']), 0)\n",
    "        decoder_predict = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper_predict, initial_state = decoder_cell.zero_state(dtype=tf.float32,batch_size=batch_size),\n",
    "            output_layer=projection_layer)\n",
    "        outputs_predict,_, __ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder_predict, maximum_iterations=seq_max_len * 2)\n",
    "    translations = outputs_predict.sample_id\n",
    "\n",
    "    # calculate loss\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=logits)\n",
    "    train_loss = (tf.reduce_sum(crossent * target_weights) /\n",
    "        batch_size)\n",
    "    \n",
    "    optimizer_ori = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    trainable_params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, trainable_params)\n",
    "    clip_gradients, _ = tf.clip_by_global_norm(gradients, max_grad)\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = optimizer_ori.apply_gradients(\n",
    "            zip(clip_gradients, trainable_params), global_step=global_step)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(train_loss)\n",
    "    #trainop = tflearn.TrainOp(loss=train_loss, optimizer=optimizer,\n",
    "    #                          metric=train_loss, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from align_and_translate_char_50000/align_and_translate_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from align_and_translate_char_50000/align_and_translate_model\n"
     ]
    }
   ],
   "source": [
    "saver.restore(session,'align_and_translate_char_50000/align_and_translate_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = \"You mother fucker , i will kill you someday .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent = 'However, they do not fall on to hard ground but on to empty cardboard boxes covered with a mattress.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = \"\"\"On the substance, it was a performance that quickly emboldened white nationalist groups and appeared certain to heighten racial tensions and fear in the country.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = \" Teaming up with Chinese search engine Baidu, online travel agency Ctrip has launched a new translation service for Chinese tourists abroad.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent = [i.lower() for i in jieba.cut(sent) if i.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents = [en2ind.get(i,en2ind['<unk>']) for i in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents = tf.contrib.keras.preprocessing.sequence.pad_sequences([sents],seq_max_len,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tran = session.run([translations],feed_dict={x:np.repeat(sents,4,axis=0),x_len:[35] * 4, x_real_len:[sum(sents[0] > 0) + 1] * 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与中国搜索引擎的联合，在线旅游社的网络旅游社已经推出了一项新的翻译服务。。）。 1\n",
      "与中国搜索引擎的联合，在线旅游社的网络公司已经开始了国外游客的新翻译服务。。 1\n",
      "与中国搜索引擎的联合，在线旅行社的网络旅游社已经开始了一项新的翻译服务。。 1\n",
      "百度与中国搜索引擎的同伙，在线旅游社团合作已经启动了国外的中国游客的新翻译服务。 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for i,j in  Counter(''.join([ind2ch.get(i,'') for i in j]) for j in tran[0]).most_common(5):\n",
    "    print i,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regx = re.compile('(?<![\\d.])(?!\\.\\.)'\n",
    "                  '(?<![\\d.][eE][+-])(?<![\\d.][eE])(?<!\\d[.,])'\n",
    "                  '' #---------------------------------\n",
    "                  '([+-]?)'\n",
    "                  '(?![\\d,]*?\\.[\\d,]*?\\.[\\d,]*?)'\n",
    "                  '(?:0|,(?=0)|(?<!\\d),)*'\n",
    "                  '(?:'\n",
    "                  '((?:\\d(?!\\.[1-9])|,(?=\\d))+)[.,]?'\n",
    "                  '|\\.(0)'\n",
    "                  '|((?<!\\.)\\.\\d+?)'\n",
    "                  '|([\\d,]+\\.\\d+?))'\n",
    "                  '0*'\n",
    "                  '' #---------------------------------\n",
    "                  '(?:'\n",
    "                  '([eE][+-]?)(?:0|,(?=0))*'\n",
    "                  '(?:'\n",
    "                  '(?!0+(?=\\D|\\Z))((?:\\d(?!\\.[1-9])|,(?=\\d))+)[.,]?'\n",
    "                  '|((?<!\\.)\\.(?!0+(?=\\D|\\Z))\\d+?)'\n",
    "                  '|([\\d,]+\\.(?!0+(?=\\D|\\Z))\\d+?))'\n",
    "                  '0*'\n",
    "                  ')?'\n",
    "                  '' #---------------------------------\n",
    "                  '(?![.,]?\\d)')\n",
    "\n",
    "\n",
    "\n",
    "simpler_regex = re.compile('(?<![\\d.])0*(?:'\n",
    "                           '(\\d+)\\.?|\\.(0)'\n",
    "                           '|(\\.\\d+?)|(\\d+\\.\\d+?)'\n",
    "                           ')0*(?![\\d.])')\n",
    "\n",
    "\n",
    "def split_outnumb(string, regx=regx, a=0):\n",
    "    excluded_pos = [x for mat in regx.finditer(string) for x in range(*mat.span()) if string[x]=='.']\n",
    "    li = []\n",
    "    for xdot in (x for x,c in enumerate(string) if c=='.' and x not in excluded_pos):\n",
    "        li.append(string[a:xdot])\n",
    "        a = xdot + 1\n",
    "    li.append(string[a:])\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On the substance, it was a performance that quickly emboldened white nationalist groups',\n",
       " 'And appeared certainto heighten racial tensions and fear in the country',\n",
       " '']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"\"\"On the substance, it was a performance that quickly emboldened white nationalist groups.And appeared certain\n",
    "to heighten racial tensions and fear in the country.\"\"\"\n",
    "sent = filter(lambda x:x != \"\\n\" and x != \"\\t\",sent)\n",
    "split_outnumb(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
